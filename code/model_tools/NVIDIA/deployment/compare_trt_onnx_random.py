import os
import json
import argparse
import torch
import numpy as np
import onnxruntime as ort
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
trt.init_libnvinfer_plugins(TRT_LOGGER, "")
trt_runtime = trt.Runtime(TRT_LOGGER)
# ========================================
# å·¥å…·å‡½æ•°
# ========================================
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    a = a.flatten()
    b = b.flatten()
    eps = 1e-8
    a_norm = np.linalg.norm(a)
    b_norm = np.linalg.norm(b)
    if a_norm < eps or b_norm < eps:
        return 0.0
    return float(np.dot(a, b) / (a_norm * b_norm + eps))

def compare_outputs(name: str, ref: np.ndarray, pred: np.ndarray):
    if ref.shape != pred.shape:
        print(f"{name:<20}: [WARN] Shape mismatch: Ref {ref.shape} vs Pred {pred.shape}")
        min_len = min(ref.size, pred.size)
        ref = ref.flatten()[:min_len]
        pred = pred.flatten()[:min_len]
    sim = cosine_similarity(ref, pred)
    mse = np.mean((ref.flatten() - pred.flatten()) ** 2)
    print(f"{name:<20}: Cosine = {sim:.6f}, MSE = {mse:.3e}")

# ========================================
# è‡ªåŠ¨åŠ è½½æˆ–ç”Ÿæˆè¾“å…¥
# ========================================
def load_or_random_input(sess_or_engine, is_trt=False, npy_dir=None):
    input_map = {}
    os.makedirs(npy_dir, exist_ok=True)
    input_names = []

    if is_trt:
        for name in sess_or_engine:
            if sess_or_engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                input_names.append(name)
    else:
        input_names = [inp.name for inp in sess_or_engine.get_inputs()]

    print("\n[ðŸ“¦ æž„é€ è¾“å…¥ä¿¡æ¯]")

    for name in input_names:
        npy_path = os.path.join(npy_dir, f"{name}.npy")
        if os.path.exists(npy_path):
            arr = np.load(npy_path)
            print(f" â”œâ”€ {name:<30} â† ä»Žæ–‡ä»¶åŠ è½½ {arr.shape}")
        else:
            print(f" â”œâ”€ [âš ï¸ ç¼ºå¤±] {npy_path}ï¼Œè‡ªåŠ¨ç”Ÿæˆéšæœºè¾“å…¥")
            if is_trt:
                shape = sess_or_engine.get_tensor_shape(name)
                shape = [1 if s <= 0 else s for s in shape]
            else:
                # ONNX æ¨¡åž‹
                input_obj = next(inp for inp in sess_or_engine.get_inputs() if inp.name == name)
                shape = [1 if isinstance(s, str) or s is None else s for s in input_obj.shape]
            print(f" â”‚   â†’ éšæœº shape = {shape}")
            arr = np.random.uniform(-1, 1, size=shape).astype(np.float32)
            np.save(npy_path, arr)
            print(f" â”‚   âœ… å·²ä¿å­˜éšæœºè¾“å…¥ {npy_path}")
        input_map[name] = arr

    print(f"[âœ… è¾“å…¥å…± {len(input_map)} ä¸ª]\n")
    return input_map

# ========================================
# ONNX æŽ¨ç†
# ========================================
def run_onnx_inference(model_path, input_map):
    sess = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider'])
    outputs = sess.run(None, input_map)
    return dict(zip([o.name for o in sess.get_outputs()], outputs))

# ========================================
# TRT æŽ¨ç†
# ========================================
def run_trt_inference(engine_path, input_map):

    with open(engine_path, 'rb') as f:
        engine = trt_runtime.deserialize_cuda_engine(f.read())
    context = engine.create_execution_context()
    stream = cuda.Stream()

    # è®¾ç½®åŠ¨æ€ shape
    for name in engine:
        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT and name in input_map:
            arr = input_map[name]
            shp = tuple(int(x) for x in arr.shape)
            context.set_input_shape(name, shp)

    bindings = []
    io_desc = {}

    for name in engine:
        mode = engine.get_tensor_mode(name)
        dtype = trt.nptype(engine.get_tensor_dtype(name))
        shape = tuple(max(1, int(d)) for d in context.get_tensor_shape(name))
        size = int(np.prod(shape))
        host = cuda.pagelocked_empty(size, dtype)
        dev = cuda.mem_alloc(host.nbytes)
        io_desc[name] = {"host": host, "dev": dev, "shape": shape, "dtype": dtype, "mode": mode}
        bindings.append(int(dev))

    # æ‹·è´è¾“å…¥
    for name, desc in io_desc.items():
        if desc["mode"] == trt.TensorIOMode.INPUT:
            np.copyto(desc["host"], input_map[name].astype(desc["dtype"]).flatten())
            cuda.memcpy_htod_async(desc["dev"], desc["host"], stream)

    context.execute_v2(bindings)

    # å–è¾“å‡º
    trt_outputs = {}
    for name, desc in io_desc.items():
        if desc["mode"] == trt.TensorIOMode.OUTPUT:
            cuda.memcpy_dtoh_async(desc["host"], desc["dev"], stream)
    stream.synchronize()

    for name, desc in io_desc.items():
        if desc["mode"] == trt.TensorIOMode.OUTPUT:
            trt_outputs[name] = desc["host"].reshape(desc["shape"])
    return trt_outputs

# ========================================
# ä¸»é€»è¾‘
# ========================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--onnx", required=True, help="Path to ONNX model")
    parser.add_argument("--trt", required=True, help="Path to TensorRT engine")
    parser.add_argument("--inputs", default="./inputs_npy", help="Folder for npy inputs (auto-generate if missing)")
    args = parser.parse_args()

    # åŠ è½½ ONNX session
    onnx_sess = ort.InferenceSession(args.onnx, providers=['CUDAExecutionProvider'])
    # åŠ è½½/ç”Ÿæˆè¾“å…¥
    input_map = load_or_random_input(onnx_sess, is_trt=False, npy_dir=args.inputs)

    print("\n[ðŸ”¹ Run ONNX]")
    onnx_out = run_onnx_inference(args.onnx, input_map)

    print("\n[ðŸ”¹ Run TensorRT]")
    # trt_input = load_or_random_input(trt.Runtime(trt.Logger(trt.Logger.ERROR)).deserialize_cuda_engine(open(args.trt, "rb").read()), is_trt=True, npy_dir=args.inputs)
    trt_out = run_trt_inference(args.trt, input_map)

    print("\n[ðŸ” Compare Outputs]")
    common = set(onnx_out.keys()) & set(trt_out.keys())
    for k in common:
        compare_outputs(k, onnx_out[k], trt_out[k])
