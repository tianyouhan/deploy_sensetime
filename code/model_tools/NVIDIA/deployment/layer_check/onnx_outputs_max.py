import onnx
import onnxruntime as ort
import numpy as np
import os
import json
from datetime import datetime

FP16_MAX = 65504.0
onnx_path = "/mnt/data/hantianyou/road_compare_tool/cast_onnx/jc/lidar-branch.onnx"
input_dir = "/mnt/data/hantianyou/lidar-branch/inputs"
output_json_path = f"onnx_max_outputs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
top_n = 20

# -----------------------------
# åŠ è½½ ONNX
# -----------------------------
import onnx
from onnx import shape_inference, numpy_helper

# ===== å…ˆåšç±»åž‹/å½¢çŠ¶æŽ¨æ–­ =====
model = onnx.load(onnx_path)
inferred = shape_inference.infer_shapes(model, strict_mode=True)

# å»ºç«‹ name -> (elem_type, shape) ç´¢å¼•
def make_vi_map(g):
    vi_map = {}
    def put_vi(vi):
        if vi and vi.name and vi.type and vi.type.tensor_type:
            tt = vi.type.tensor_type
            elem = tt.elem_type  # onnx.TensorProto.DataType æžšä¸¾
            # å½¢çŠ¶å¯é€‰ï¼›æˆ‘ä»¬åªå…³å¿ƒç±»åž‹ï¼Œå½¢çŠ¶ç»™ None ä¹Ÿè¡Œ
            vi_map[vi.name] = (elem, None)

    for vi in list(g.value_info) + list(g.input) + list(g.output):
        put_vi(vi)

    # å¸¸é‡åˆå§‹å€¼ï¼ˆinitializerï¼‰ä¹Ÿèƒ½æä¾›ç±»åž‹
    for init in g.initializer:
        vi_map[init.name] = (init.data_type, None)

    return vi_map

vi_map = make_vi_map(inferred.graph)

# å·²æœ‰çš„ graph outputs åé›†åˆï¼Œé¿å…é‡å¤æ·»åŠ 
existing_outs = {o.name for o in model.graph.output}

# ===== é€èŠ‚ç‚¹è¾“å‡ºï¼ŒæŒ‰çœŸå®žç±»åž‹è¿½åŠ ä¸º graph output =====
added = 0
for node in model.graph.node:
    for out in node.output:
        if out in existing_outs:
            continue
        # ä»ŽæŽ¨æ–­/å·²æœ‰ä¿¡æ¯é‡Œå–çœŸå®žç±»åž‹ï¼›å–ä¸åˆ°å°±è·³è¿‡ï¼ˆæˆ–æŒ‰éœ€é»˜è®¤ FLOATï¼‰
        dtype_shape = vi_map.get(out, None)
        if dtype_shape is None:
            # å°è¯•å·²çŸ¥ç‰¹æ®Šç®—å­å…œåº•ï¼ˆå¯é€‰ï¼‰
            if node.op_type in ("Shape", "ArgMax", "ArgMin", "NonZero", "TopK"):
                elem_type = onnx.TensorProto.INT64
            elif node.op_type in ("Greater", "Less", "Equal", "And", "Or", "Not"):
                elem_type = onnx.TensorProto.BOOL
            else:
                # å¦‚æžœä½ ç¡®å®žæƒ³â€œå…¨éƒ½æ‹¿åˆ°â€ï¼Œå¯ä»¥å‹‰å¼ºé»˜è®¤ FLOATï¼Œä½†**å¯èƒ½å†æ¬¡è§¦å‘ç±»åž‹ä¸åŒ¹é…**
                # elem_type = onnx.TensorProto.FLOAT
                # ä¸ºäº†ç¨³å¦¥ï¼Œè¿™é‡Œé€‰æ‹©è·³è¿‡ï¼Œæˆ–è€…è®°å½•ä¸€ä¸‹åå­—æ—¥å¿—
                # print(f"Skip add output (unknown dtype): {out} from {node.op_type}")
                continue
            shape = None
        else:
            elem_type, shape = dtype_shape

        model.graph.output.append(
            onnx.helper.make_tensor_value_info(out, elem_type, shape)  # shape å¯ä»¥ None
        )
        existing_outs.add(out)
        added += 1

print(f"Added {added} intermediate outputs with correct dtypes.")

tmp_onnx_path = "tmp_all_outputs.onnx"
onnx.save(model, tmp_onnx_path)
print("âœ… Done saving tmp ONNX with correct dtypes.")

# -----------------------------
# åˆ›å»º ORT session
# -----------------------------
sess_options = ort.SessionOptions()
sess_options.log_severity_level = 3
sess = ort.InferenceSession(tmp_onnx_path, sess_options)

# å‡†å¤‡è¾“å…¥
# inputs = {}
# for fname in os.listdir(input_dir):
#     if fname.endswith(".npy"):
#         key = fname.replace(".npy", "")
#         arr = np.load(os.path.join(input_dir, fname)).astype(np.float32)
#         # if arr.ndim == 2:
#         #     arr = np.expand_dims(arr, axis=0)
#         inputs[key] = arr
inputs = {}

for subdir in os.listdir(input_dir):
    subdir_path = os.path.join(input_dir, subdir)
    if not os.path.isdir(subdir_path):
        continue

    # å– 0.npyï¼Œè‹¥ä¸å­˜åœ¨ï¼Œå–æœ€å°ç¼–å·çš„å¸§
    target_npy = os.path.join(subdir_path, "0.npy")

    if not os.path.exists(target_npy):
        # æ‰¾æœ€å°ç¼–å· .npy
        npy_files = sorted([f for f in os.listdir(subdir_path) if f.endswith(".npy")])
        if len(npy_files) == 0:
            print(f"âš ï¸ è·³è¿‡ {subdir}ï¼ˆæ—  .npy æ–‡ä»¶ï¼‰")
            continue
        target_npy = os.path.join(subdir_path, npy_files[0])

    arr = np.load(target_npy).astype(np.float32)
    inputs[subdir] = arr

    print(f"Loaded {subdir}: {os.path.basename(target_npy)}, shape={arr.shape}")

# -----------------------------
# éåŽ†è¾“å‡ºå¹¶è®¡ç®—æœ€å¤§ç»å¯¹å€¼
# -----------------------------
max_abs_list = []
output_names = [o.name for o in sess.get_outputs()]
total_outputs = len(output_names)

for idx, out_name in enumerate(output_names, 1):
    try:
        out = sess.run([out_name], inputs)[0]
        max_abs = float(np.max(np.abs(out)))
        max_abs_list.append((out_name, max_abs))
        if idx % 10 == 0 or idx == total_outputs:
            print(f"[{idx}/{total_outputs}] processed output: {out_name}, max_abs={max_abs:.6f}")
    except Exception as e:
        print(f"âš ï¸ èŠ‚ç‚¹ {out_name} æŽ¨ç†å¤±è´¥: {e}")

# æŽ’åº top N
max_abs_list.sort(key=lambda x: x[1], reverse=True)
print(f"\nðŸ” Top-{top_n} æœ€å¤§ç»å¯¹å€¼è¾“å‡ºèŠ‚ç‚¹:")
for rank, (name, val) in enumerate(max_abs_list[:top_n], 1):
    overflow_flag = " âš ï¸ FP16 æº¢å‡º" if val > FP16_MAX else ""
    print(f"{rank:>2}. {name:<60} max_abs={val:.6f}{overflow_flag}")

# -----------------------------
# ä¿å­˜ JSON
# -----------------------------
json_data = []
for idx, (name, val) in enumerate(max_abs_list, 1):
    if idx % 50 == 0 or idx == total_outputs:
        print(f"[{idx}/{total_outputs}] preparing JSON entry for: {name}")
    json_data.append({"name": name, "max_abs": val})

with open(output_json_path, "w") as f:
    json.dump(json_data, f, indent=2)

print(f"âœ… å·²ä¿å­˜åˆ° {output_json_path}")
