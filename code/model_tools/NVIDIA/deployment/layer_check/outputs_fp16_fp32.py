import tensorrt as trt
import numpy as np
import os
import json
import io
import base64
import pycuda.driver as cuda
import pycuda.autoinit
import argparse

# -----------------------------
# å‘½ä»¤è¡Œå‚æ•° / é…ç½®å¼€å…³
# -----------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--markAllOutputs", action="store_true", help="æ ‡è®°æ‰€æœ‰ä¸­é—´å±‚è¾“å‡º")
args = parser.parse_args()

MARK_ALL_OUTPUTS = args.markAllOutputs  # ä¹Ÿå¯ä»¥ç›´æ¥æ”¹æˆ True/False æµ‹è¯•
# -----------------------------
# é…ç½®
# -----------------------------
onnx_path = "/mnt/data/hantianyou/occ_pred/spetr_occ_1212.onnx"
engine_path = "/mnt/data/hantianyou/occ_pred/spetr_occ_1212_fp16_select.trt"
output_json_path = "/mnt/data/hantianyou/road_compare_tool/layer_check/save_outputs/outputs_fp16_select.json"
input_dir = "/mnt/data/hantianyou/occ_pred/spetr_occ_input_1212_10000"
# fp32_layers_set = [
#     # "/ScatterND",
#     # "/ScatterND_1",
#     # ä»¥ä¸‹ä¸ºæ‰‹åŠ¨æŒ‡å®šçš„ LayerNorm ä¸ Attention å±‚ï¼Œå¼ºåˆ¶ä½¿ç”¨ FP32 ç²¾åº¦
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/output_proj/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/output_proj/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/Add_12",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/output_proj/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/output_proj/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/Add_12",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/output_proj/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/output_proj/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/Add_12",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/output_proj/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/output_proj/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/Add_12",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/ffns.0/layers/layers.0/layers.0.0/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/ffns.0/layers/layers.0/layers.0.0/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/ffns.0/layers/layers.0/layers.0.0/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/ffns.0/layers/layers.0/layers.0.0/MatMul",
#     "/model/pts_bbox_head/ego_pose_memory/ln/Pow",
#     "/model/pts_bbox_head/ego_pose_memory/ln/ReduceMean_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/cam_embed/cam_embed.4/Pow",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/cam_embed/cam_embed.4/ReduceMean_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/cam_embed/cam_embed.4/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/cam_embed/cam_embed.4/Sqrt",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/cam_embed/cam_embed.4/Add_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.0/attentions.1/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/cam_embed/cam_embed.4/Pow",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/cam_embed/cam_embed.4/ReduceMean_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/cam_embed/cam_embed.4/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/cam_embed/cam_embed.4/Sqrt",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/cam_embed/cam_embed.4/Add_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.1/attentions.1/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/cam_embed/cam_embed.4/Pow",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/cam_embed/cam_embed.4/ReduceMean_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/cam_embed/cam_embed.4/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/cam_embed/cam_embed.4/Sqrt",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/cam_embed/cam_embed.4/Add_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.2/attentions.1/MatMul",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/cam_embed/cam_embed.4/Pow",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/cam_embed/cam_embed.4/ReduceMean_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/cam_embed/cam_embed.4/Add",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/cam_embed/cam_embed.4/Sqrt",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/cam_embed/cam_embed.4/Add_1",
#     "/model/pts_bbox_head/transformer/decoder/layers.3/attentions.1/MatMul",
# ]

fp32_layers_set = [
    # "/model/occ3d_head/predicter/predicter.0/Add",
    # "/model/occ3d_head/occ_head/occ_head.0/Add",
    "/model/occ3d_head/predicter/predicter.1/Softplus",
    "/model/occ3d_head/occ_head/occ_head.1/Softplus",
]

# -----------------------------
# åˆå§‹åŒ– TRT
# -----------------------------
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
trt.init_libnvinfer_plugins(TRT_LOGGER, "")

builder = trt.Builder(TRT_LOGGER)
network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
network = builder.create_network(network_flags)

parser = trt.OnnxParser(network, TRT_LOGGER)
with open(onnx_path, "rb") as f:
    if not parser.parse(f.read()):
        for i in range(parser.num_errors):
            print(parser.get_error(i))
        raise RuntimeError("ONNX parse failed")

# -----------------------------
# æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦å¯¼å‡ºä¸­é—´å±‚
# -----------------------------
if MARK_ALL_OUTPUTS:
    print("ğŸ”¹ æ ‡è®°æ‰€æœ‰ä¸­é—´å±‚è¾“å‡ºä¸º network output ...")
    for i in range(network.num_layers):
        layer = network.get_layer(i)
        for j in range(layer.num_outputs):
            tensor = layer.get_output(j)
            if tensor and not tensor.is_network_output:
                network.mark_output(tensor)
    print(f"âœ… å·²æ ‡è®° {network.num_outputs} ä¸ªè¾“å‡ºå¼ é‡")
else:
    print("âš™ï¸ ä»…ä¿ç•™æœ€ç»ˆè¾“å‡ºå±‚ï¼Œä¸å¯¼å‡ºä¸­é—´å±‚ã€‚")

# -----------------------------
# BuilderConfig
# -----------------------------
config = builder.create_builder_config()
config.set_flag(trt.BuilderFlag.FP16)
if hasattr(trt.BuilderFlag, "TF32"):
    config.clear_flag(trt.BuilderFlag.TF32)

# -----------------------------
# è®¾ç½®æ•æ„Ÿå±‚ FP32
# -----------------------------
for i in range(network.num_layers):
    layer = network.get_layer(i)
    set_fp32 = False

    # æ‰‹åŠ¨åˆ—è¡¨åŒ¹é…
    normalized_layer_name = layer.name.replace('/', '.').replace('', '').strip()
    for name in fp32_layers_set:
        normalized_name = name.replace('/', '.').replace('', '').strip()
        if normalized_name in normalized_layer_name:
            set_fp32 = True
            break

    if set_fp32:
        for j in range(layer.num_outputs):
            try:
                layer.precision = trt.DataType.FLOAT
                layer.set_output_type(j, trt.DataType.FLOAT)
                print(f"Found node {layer.name} ({layer.type}) to FP32")
            except Exception as e:
                print(f"âš ï¸ Cannot set FP32 for layer {layer.name}: {e}")
scatter_keywords = ["Scatter", "scatter", "ScatterND", "scatter_nd", "ScatterElements"]

# print("\n=== Network layers (name | type | precision) ===")
# for i in range(network.num_layers):
#     layer = network.get_layer(i)
#     try:
#         print(f"{i:03d}: name='{layer.name}' type='{str(layer.type)}' precision='{layer.precision}' outputs={layer.num_outputs}")
#     except Exception:
#         print(f"{i:03d}: name='{layer.name}' type='{str(layer.type)}' (error reading precision)")

# # -----------------------------
# # é…ç½®åŠ¨æ€shapeä¼˜åŒ–profile
# # -----------------------------
# print("ğŸ”§ é…ç½®åŠ¨æ€shapeä¼˜åŒ–profile...")
# profile = builder.create_optimization_profile()

# é¦–å…ˆåŠ è½½è¾“å…¥æ•°æ®æ¥è·å–å®é™…shape
inputs = {}
input_shapes = {}

for name in os.listdir(input_dir):
    subdir = os.path.join(input_dir, name)
    if not os.path.isdir(subdir):
        continue

    npy_path = os.path.join(subdir, "0.npy")
    bin_path = os.path.join(subdir, "0.bin")
    shape_path = os.path.join(subdir, "shape.json")

    if os.path.exists(npy_path):
        # ---------- npy ----------
        arr = np.load(npy_path)
        if arr.dtype != np.float32:
            arr = arr.astype(np.float32)

        inputs[name] = arr
        input_shapes[name] = arr.shape
        print(f"âœ… åŠ è½½ npy è¾“å…¥: {name}, shape={arr.shape}")

    elif os.path.exists(bin_path):
        # ---------- bin ----------
        if not os.path.exists(shape_path):
            print(f"âŒ {subdir} ä¸­å­˜åœ¨ 0.bin ä½†ç¼ºå°‘ shape.jsonï¼Œè·³è¿‡")
            continue

        with open(shape_path, "r") as f:
            meta = json.load(f)

        dtype = np.dtype(meta["dtype"])
        shape = tuple(meta["shape"])

        arr = np.fromfile(bin_path, dtype=dtype).reshape(shape)

        if arr.dtype != np.float32:
            arr = arr.astype(np.float32)

        inputs[name] = arr
        input_shapes[name] = arr.shape
        print(f"âœ… åŠ è½½ bin è¾“å…¥: {name}, shape={arr.shape}")

    else:
        print(f"âš ï¸ è·³è¿‡ {subdir}ï¼Œæ—¢æ²¡æœ‰ 0.npy ä¹Ÿæ²¡æœ‰ 0.bin")
# input_data = {}
# input_shapes = {}
# for fname in os.listdir(input_dir):
#     if fname.endswith(".npy"):
#         key = fname.replace(".npy", "")
#         arr = np.load(os.path.join(input_dir, fname)).astype(np.float32)
#         # if arr.ndim == 2:
#         #     arr = np.expand_dims(arr, axis=0)
#         input_data[key] = arr
#         input_shapes[key] = arr.shape
#         print(f"è¾“å…¥ {key}: shape={arr.shape}")

# # ä¸ºæ¯ä¸ªè¾“å…¥è®¾ç½®åŠ¨æ€èŒƒå›´
# for i in range(network.num_inputs):
#     input_tensor = network.get_input(i)
#     input_name = input_tensor.name
    
#     if input_name in input_shapes:
#         actual_shape = input_shapes[input_name]
        
#         # å¤„ç†åŠ¨æ€ç»´åº¦ï¼ˆ-1ï¼‰
#         min_shape = []
#         opt_shape = []
#         max_shape = []
        
#         for dim in actual_shape:
#             if dim == -1 or dim is None:
#                 # åŠ¨æ€ç»´åº¦ï¼Œè®¾ç½®åˆç†èŒƒå›´
#                 min_shape.append(1)      # æœ€å°
#                 opt_shape.append(8)      # æœ€ä¼˜ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
#                 max_shape.append(32)     # æœ€å¤§ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
#             else:
#                 # å›ºå®šç»´åº¦
#                 min_shape.append(dim)
#                 opt_shape.append(dim)
#                 max_shape.append(dim)
        
#         print(f"è®¾ç½®è¾“å…¥ '{input_name}' åŠ¨æ€èŒƒå›´:")
#         print(f"  min: {min_shape}")
#         print(f"  opt: {opt_shape}")
#         print(f"  max: {max_shape}")
        
#         profile.set_shape(input_name, min_shape, opt_shape, max_shape)
#     else:
#         print(f"âš ï¸ è­¦å‘Š: è¾“å…¥ '{input_name}' æœªåœ¨è¾“å…¥æ•°æ®ä¸­æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤shape")

# config.add_optimization_profile(profile)

# -----------------------------
# æ„å»º engine
# -----------------------------
print("ğŸ”¨ å¼€å§‹æ„å»º engine...")
serialized_engine = builder.build_serialized_network(network, config)
if serialized_engine is None:
    raise RuntimeError("Failed to build engine!")

with open(engine_path, "wb") as f:
    f.write(serialized_engine)
print(f"\nâœ… å·²ç”Ÿæˆæ”¯æŒåŠ¨æ€shapeçš„æ··åˆç²¾åº¦ engine: {engine_path}")

# -----------------------------
# åŠ è½½ engine å¹¶è¿›è¡Œæ¨ç†
# -----------------------------
runtime = trt.Runtime(TRT_LOGGER)
with open(engine_path, "rb") as f:
    engine = runtime.deserialize_cuda_engine(f.read())

context = engine.create_execution_context()

# -----------------------------
# åˆ†é… CUDA buffer - æ”¯æŒåŠ¨æ€shape
# -----------------------------
bindings = []
cuda_buffers = {}

print("=== å¼€å§‹åˆ†é… CUDA å†…å­˜ï¼ˆåŠ¨æ€shapeï¼‰ ===")

# æ–¹æ³•1: ä½¿ç”¨æ–°çš„ TensorRT API (æ¨è)
if hasattr(engine, 'num_io_tensors'):
    # TensorRT 8.5+ æ–° API
    num_io_tensors = engine.num_io_tensors
    print(f"ä½¿ç”¨æ–° APIï¼Œå…±æœ‰ {num_io_tensors} ä¸ª IO tensors")
    
    tensor_names = [engine.get_tensor_name(i) for i in range(num_io_tensors)]
    
    for i, name in enumerate(tensor_names):
        try:
            dtype = trt.nptype(engine.get_tensor_dtype(name))
            
            # å¯¹äºåŠ¨æ€shapeï¼Œå…ˆè®¾ç½®è¾“å…¥çš„å®é™…shape
            if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                if name in input_shapes:
                    actual_shape = input_shapes[name]
                    context.set_input_shape(name, actual_shape)
                    print(f"âœ… è®¾ç½®è¾“å…¥ '{name}' çš„shapeä¸º: {actual_shape}")
            
            # è·å–è®¾ç½®åçš„å®é™…shape
            shape = context.get_tensor_shape(name)
            
            print(f"\n--- Tensor {i}: {name} ---")
            print(f"  Shape: {shape}")
            print(f"  Dtype: {dtype}")
            
            # æ£€æŸ¥å½¢çŠ¶ä¸­æ˜¯å¦æœ‰æ— æ•ˆå€¼
            if any(dim is not None and dim <= 0 for dim in shape):
                print(f"  âš ï¸ è­¦å‘Š: å½¢çŠ¶åŒ…å«æ— æ•ˆç»´åº¦: {shape}")
                # å¯¹äºæ— æ•ˆå½¢çŠ¶ï¼Œè®¾ç½®é»˜è®¤å½¢çŠ¶ [1]
                shape = [1]
                print(f"  ä½¿ç”¨é»˜è®¤å½¢çŠ¶: {shape}")
            
            # è®¡ç®—å¤§å°
            volume = trt.volume(shape) if shape else 1
            dtype_size = np.dtype(dtype).itemsize
            size = volume * dtype_size
            
            print(f"  Volume: {volume}")
            print(f"  Dtype size: {dtype_size}")
            print(f"  Total size: {size} bytes")
            
            # æ£€æŸ¥å¤§å°æ˜¯å¦æœ‰æ•ˆ
            if size <= 0:
                print(f"  âŒ é”™è¯¯: è®¡ç®—çš„å¤§å°æ— æ•ˆ: {size}")
                # åˆ†é…æœ€å°å†…å­˜
                size = 1024  # 1KB
                print(f"  åˆ†é…æœ€å°å†…å­˜: {size} bytes")
            
            print(f"  æ­£åœ¨åˆ†é… {size} bytes å†…å­˜...")
            buffer = cuda.mem_alloc(size)
            print(f"  âœ… å†…å­˜åˆ†é…æˆåŠŸ")
            
            cuda_buffers[name] = buffer
            bindings.append(int(buffer))
            
            if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                print(f"  Input: {name}, shape: {shape}, dtype: {dtype}")
            else:
                print(f"  Output: {name}, shape: {shape}, dtype: {dtype}")
                
        except Exception as e:
            print(f"  âŒ å¤„ç†tensor {name}æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue

else:
    # TensorRT 8.4åŠä»¥ä¸‹æ—§ API
    print(f"ä½¿ç”¨æ—§ APIï¼Œå…±æœ‰ {engine.num_bindings} ä¸ª bindings")
    
    for i in range(engine.num_bindings):
        try:
            name = engine.get_binding_name(i)
            dtype = trt.nptype(engine.get_binding_dtype(i))
            
            # å¯¹äºåŠ¨æ€shapeï¼Œå…ˆè®¾ç½®è¾“å…¥çš„å®é™…shape
            if engine.binding_is_input(i):
                if name in input_shapes:
                    actual_shape = input_shapes[name]
                    context.set_binding_shape(i, actual_shape)
                    print(f"âœ… è®¾ç½®è¾“å…¥ '{name}' çš„shapeä¸º: {actual_shape}")
            
            # è·å–è®¾ç½®åçš„å®é™…shape
            shape = context.get_binding_shape(i)
            
            print(f"\n--- Binding {i}: {name} ---")
            print(f"  Shape: {shape}")
            print(f"  Dtype: {dtype}")
            
            # æ£€æŸ¥å½¢çŠ¶ä¸­æ˜¯å¦æœ‰æ— æ•ˆå€¼
            if any(dim is not None and dim <= 0 for dim in shape):
                print(f"  âš ï¸ è­¦å‘Š: å½¢çŠ¶åŒ…å«æ— æ•ˆç»´åº¦: {shape}")
                # å¯¹äºæ— æ•ˆå½¢çŠ¶ï¼Œè®¾ç½®é»˜è®¤å½¢çŠ¶ [1]
                shape = [1]
                print(f"  ä½¿ç”¨é»˜è®¤å½¢çŠ¶: {shape}")
            
            # è®¡ç®—å¤§å°
            volume = trt.volume(shape) if shape else 1
            dtype_size = np.dtype(dtype).itemsize
            size = volume * dtype_size
            
            print(f"  Volume: {volume}")
            print(f"  Dtype size: {dtype_size}")
            print(f"  Total size: {size} bytes")
            
            # æ£€æŸ¥å¤§å°æ˜¯å¦æœ‰æ•ˆ
            if size <= 0:
                print(f"  âŒ é”™è¯¯: è®¡ç®—çš„å¤§å°æ— æ•ˆ: {size}")
                # åˆ†é…æœ€å°å†…å­˜
                size = 1024  # 1KB
                print(f"  åˆ†é…æœ€å°å†…å­˜: {size} bytes")
            
            print(f"  æ­£åœ¨åˆ†é… {size} bytes å†…å­˜...")
            buffer = cuda.mem_alloc(size)
            print(f"  âœ… å†…å­˜åˆ†é…æˆåŠŸ")
            
            cuda_buffers[name] = buffer
            bindings.append(int(buffer))
            
            if engine.binding_is_input(i):
                print(f"  Input: {name}, shape: {shape}, dtype: {dtype}")
            else:
                print(f"  Output: {name}, shape: {shape}, dtype: {dtype}")
                
        except Exception as e:
            print(f"  âŒ å¤„ç†binding {i} ({name})æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue

print(f"\n=== å†…å­˜åˆ†é…å®Œæˆ ===")
print(f"æˆåŠŸåˆ†é…äº† {len(cuda_buffers)} ä¸ª buffers")

# -----------------------------
# è½½å…¥è¾“å…¥
# -----------------------------
print("\n=== åŠ è½½è¾“å…¥æ•°æ® ===")
for key, arr in inputs.items():
    if key not in cuda_buffers:
        print(f"âš ï¸ è·³è¿‡æœªåœ¨engineä¸­æ‰¾åˆ°çš„è¾“å…¥: {key}")
        continue
    
    try:
        cuda.memcpy_htod(cuda_buffers[key], arr)
        print(f"âœ… å·²åŠ è½½è¾“å…¥: {key}, shape: {arr.shape}")
    except Exception as e:
        print(f"âŒ åŠ è½½è¾“å…¥ {key} å¤±è´¥: {e}")

# -----------------------------
# æ‰§è¡Œæ¨ç†
# -----------------------------
print("\n=== å¼€å§‹æ¨ç† ===")
try:
    # æ£€æŸ¥æ‰€æœ‰è¾“å…¥çš„å½¢çŠ¶æ˜¯å¦æœ‰æ•ˆ
    if hasattr(engine, 'num_io_tensors'):
        # æ–°APIï¼šæ£€æŸ¥æ‰€æœ‰è¾“å…¥tensorçš„å½¢çŠ¶
        for i in range(engine.num_io_tensors):
            name = engine.get_tensor_name(i)
            if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                shape = context.get_tensor_shape(name)
                print(f"è¾“å…¥ '{name}' æ¨ç†å‰shape: {shape}")
    else:
        # æ—§APIï¼šæ£€æŸ¥æ‰€æœ‰bindingçš„å½¢çŠ¶
        for i in range(engine.num_bindings):
            if engine.binding_is_input(i):
                name = engine.get_binding_name(i)
                shape = context.get_binding_shape(i)
                print(f"è¾“å…¥ '{name}' æ¨ç†å‰shape: {shape}")
    
    # æ‰§è¡Œæ¨ç†
    if hasattr(context, 'execute_v2'):
        success = context.execute_v2(bindings)
    else:
        success = context.execute_v1(bindings)
    
    print("âœ… æ¨ç†å®Œæˆ")
except Exception as e:
    print(f"âŒ æ¨ç†å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# -----------------------------
# æ‹·è´è¾“å‡º - æ”¯æŒåŠ¨æ€shape
# -----------------------------
outputs = {}
missing_outputs = []

print("\n=== å¼€å§‹æ‹·è´è¾“å‡º ===")

if hasattr(engine, 'num_io_tensors'):
    # æ–° API
    tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]
    for name in tensor_names:
        if engine.get_tensor_mode(name) == trt.TensorIOMode.OUTPUT:
            if name not in cuda_buffers:
                print(f"âŒ è¾“å‡ºtensor '{name}' ä¸åœ¨cuda_buffersä¸­ï¼Œè·³è¿‡")
                missing_outputs.append(name)
                continue
                
            try:
                dtype = trt.nptype(engine.get_tensor_dtype(name))
                shape = context.get_tensor_shape(name)  # è·å–åŠ¨æ€æ¨ç†åçš„å®é™…shape
                print(f"æ‹·è´è¾“å‡º: {name}, shape: {shape}, dtype: {dtype}")
                
                host_arr = np.empty(shape, dtype=dtype)
                cuda.memcpy_dtoh(host_arr, cuda_buffers[name])
                outputs[name] = host_arr
                print(f"âœ… æˆåŠŸæ‹·è´è¾“å‡º: {name}")
            except Exception as e:
                print(f"âŒ æ‹·è´è¾“å‡º {name} å¤±è´¥: {e}")
else:
    # æ—§ API
    for i in range(engine.num_bindings):
        name = engine.get_binding_name(i)
        if not engine.binding_is_input(i):
            if name not in cuda_buffers:
                print(f"âŒ è¾“å‡ºtensor '{name}' ä¸åœ¨cuda_buffersä¸­ï¼Œè·³è¿‡")
                missing_outputs.append(name)
                continue
                
            try:
                dtype = trt.nptype(engine.get_binding_dtype(i))
                shape = context.get_binding_shape(i)  # è·å–åŠ¨æ€æ¨ç†åçš„å®é™…shape
                print(f"æ‹·è´è¾“å‡º: {name}, shape: {shape}, dtype: {dtype}")
                
                host_arr = np.empty(shape, dtype=dtype)
                cuda.memcpy_dtoh(host_arr, cuda_buffers[name])
                outputs[name] = host_arr
                print(f"âœ… æˆåŠŸæ‹·è´è¾“å‡º: {name}")
            except Exception as e:
                print(f"âŒ æ‹·è´è¾“å‡º {name} å¤±è´¥: {e}")

if missing_outputs:
    print(f"\nâš ï¸ è­¦å‘Š: è·³è¿‡äº† {len(missing_outputs)} ä¸ªç¼ºå¤±çš„è¾“å‡ºtensor")
    for name in missing_outputs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  - {name}")

# -----------------------------
# è¾“å‡ºæ£€æŸ¥ + è¶…å‡ºFP16èŒƒå›´å†™log
# -----------------------------
log_path = "/mnt/data/hantianyou/occ_pred/fp16_abnormal.log"
list_path = "/mnt/data/hantianyou/occ_pred/fp16_abnormal_layers.txt"

naninf_layers = set()

with open(log_path, "w", encoding="utf-8") as logf:
    print(f"\n=== æ¨ç†è¾“å‡ºæ£€æŸ¥ ===")
    print(f"âš ï¸ ä»…è®°å½• NaN/Inf æˆ–è¶…å‡º FP16 èŒƒå›´çš„å±‚åˆ°: {log_path}")

    for name, arr in outputs.items():
        if arr.size == 0:
            continue

        max_val = arr.max() if arr.size > 0 else 0
        min_val = arr.min() if arr.size > 0 else 0

        msg = f"[{name}] dtype={arr.dtype} min={min_val:.6e} max={max_val:.6e}"
        print(msg)

        # # æ£€æŸ¥ FP16 æº¢å‡º
        # if arr.dtype == np.float16 and (max_val > 65504 or min_val < -65504):
        #     msg = f"âš ï¸ [{name}] è¶…å‡º FP16 èŒƒå›´: max={max_val:.6f}, min={min_val:.6f}"
        #     print(msg)
        #     logf.write(msg + "\n")

        # æ£€æŸ¥ NaN / Inf
        if np.isnan(arr).any() or np.isinf(arr).any():
            nan_count = int(np.isnan(arr).sum())
            inf_count = int(np.isinf(arr).sum())
            msg = f"âš ï¸ [{name}] æ£€æµ‹åˆ° NaN/Inf: NaN={nan_count}, Inf={inf_count}"
            print(msg)
            logf.write(msg + "\n")
            naninf_layers.add(name)

print(f"\nâœ… å¼‚å¸¸å±‚æ—¥å¿—å·²ä¿å­˜åˆ°: {log_path}")

# åªä¿å­˜å« NaN/Inf çš„å±‚åä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
if naninf_layers:
    list_str = "fp32_layers_set = [\n"
    for nm in sorted(naninf_layers):
        list_str += f'    "{nm}",\n'
    list_str += "]\n"

    with open(list_path, "w", encoding="utf-8") as lf:
        lf.write(list_str)

    print(f"âœ… å« NaN/Inf çš„å±‚åˆ—è¡¨å·²ä¿å­˜åˆ°: {list_path}")

# -----------------------------
# ä¿å­˜ JSON + base64
# -----------------------------
json_dict = {"lst": [[None, [{"outputs": {}}]]]}
for name, arr in outputs.items():
    bio = io.BytesIO()
    np.save(bio, arr, allow_pickle=True)
    b64str = base64.b64encode(bio.getvalue()).decode("ascii")
    json_dict["lst"][0][1][0]["outputs"][name] = {
        "dtype": str(arr.dtype),
        "values": {"array": b64str}
    }

with open(output_json_path, "w") as f:
    json.dump(json_dict, f)

print(f"\nâœ… å·²ä¿å­˜æ··åˆç²¾åº¦è¾“å‡ºåˆ° {output_json_path}")
print(f"âœ… æ€»å…±ä¿å­˜äº† {len(outputs)} ä¸ªè¾“å‡ºå¼ é‡")
print(f"âš ï¸ è·³è¿‡äº† {len(missing_outputs)} ä¸ªç¼ºå¤±çš„è¾“å‡ºå¼ é‡")